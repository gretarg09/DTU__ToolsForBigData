{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Read the image, convert it into grayscale, and make in binary image for threshold value of 1.\n",
    "img = cv2.imread('frame.png')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "_,thresh = cv2.threshold(gray,1,255,cv2.THRESH_BINARY)\n",
    "\n",
    "#Now find contours in it. There will be only one object, so find bounding rectangle for it.\n",
    "\n",
    "contours = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnt = contours[0]\n",
    "x,y,w,h = cv2.boundingRect(cnt)\n",
    "\n",
    "#Now crop the image, and save it into another file.\n",
    "\n",
    "crop = img[y:y+h,x:x+w]\n",
    "cv2.imwrite('framemod.png',crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def autocrop(image, threshold=0):\n",
    "    \"\"\"Crops any edges below or equal to threshold\n",
    "\n",
    "    Crops blank image to 1x1.\n",
    "\n",
    "    Returns cropped image.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 3:\n",
    "        flatImage = np.max(image, 2)\n",
    "    else:\n",
    "        flatImage = image\n",
    "    assert len(flatImage.shape) == 2\n",
    "\n",
    "    rows = np.where(np.max(flatImage, 0) > threshold)[0]\n",
    "    if rows.size:\n",
    "        cols = np.where(np.max(flatImage, 1) > threshold)[0]\n",
    "        image = image[cols[0]: cols[-1] + 1, rows[0]: rows[-1] + 1]\n",
    "    else:\n",
    "        image = image[:1, :1]\n",
    "\n",
    "    return image\n",
    "\n",
    "img = cv2.imread('frame.png')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "crop = autocrop(gray)\n",
    "cv2.imwrite('framemodOther.png',crop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crop_image(img,tol=0):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "img = cv2.imread('frame.png')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "crop = autocrop(gray)\n",
    "cv2.imwrite('framemodThird.png',crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image histogram equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img1 = cv2.imread('histogramNormalization/first.png',0)\n",
    "img2 = cv2.imread('histogramNormalization/second.png',0)\n",
    "# Equalization\n",
    "#img = cv2.imread('wiki.jpg',0)\n",
    "equ1 = cv2.equalizeHist(img1)\n",
    "equ2 = cv2.equalizeHist(img2)\n",
    "res = np.hstack((equ1,equ2)) #stacking images side-by-side\n",
    "cv2.imwrite('histogramNormalization/res.png',res)\n",
    "\n",
    "\n",
    "# create a CLAHE object (Arguments are optional).\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "cl1 = clahe.apply(img1)\n",
    "cl2 = clahe.apply(img2)\n",
    "res_cl = np.hstack((cl1, cl2)) #stacking images side-by-side\n",
    "cv2.imwrite('histogramNormalization/res_cl.png',res_cl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distances between hex values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function takes in a list of key value pairs (image key, hex value string) and \n",
    "# Finds all combinations of the pairs and then calculated the number of different bits between two hash strings\n",
    "# It returns a sorted list, sorted in reverse order so the pairs with the highest difference are on the top of the list\n",
    "def find_hamming_distances(tuple_pair_array):\n",
    "    distances = []\n",
    "\n",
    "    # find all combinations\n",
    "    for i in combinations(tuple_pair_array, 2):\n",
    "        distance =  get_hamming_distance(i[0][1],i[1][1])\n",
    "        distances.append((i[0],i[1],distance))\n",
    "\n",
    "    distances = sorted(distances, key =  lambda x:x[2], reverse=True)\n",
    "\n",
    "    for distance_pair in distances:\n",
    "        output = \"{}|{} - {}|{} - {}\".format(\n",
    "            distance_pair[0][0],\n",
    "            distance_pair[1][0],\n",
    "            distance_pair[0][1],\n",
    "            distance_pair[1][1],\n",
    "            distance_pair[2]\n",
    "        )\n",
    "\n",
    "        print output\n",
    "\n",
    "\n",
    "# Functions that finds number of different bits between two hash strings  \n",
    "def get_hamming_distance(hash_string1, hash_string2):\n",
    "    \"\"\"Get the number of different bits between two hash strings.\"\"\"\n",
    "    dist = 0\n",
    "    # get diff matrices from hash string\n",
    "    bits1 = hash_to_bits(hash_string1)\n",
    "    bits2 = hash_to_bits(hash_string2)\n",
    "\n",
    "    # compute distance\n",
    "    for bit1, bit2 in zip(bits1, bits2):\n",
    "        if bit1 != bit2:\n",
    "            dist += 1\n",
    "    return dist\n",
    "\n",
    "def hash_to_bits(hash_string):\n",
    "    \"\"\"Convert a hash string into the corresponding bit string.\"\"\"\n",
    "    bits = []\n",
    "    # Convert into individual hex numbers\n",
    "    hex_nums = ['0x' + hash_string[i:i+2] for i in range(0, len(hash_string), 2)]\n",
    "    for hex_num in hex_nums:\n",
    "        bit_string = bin(int(hex_num, 16))[2:].rjust(8, '0') # binary string\n",
    "        bits.append(bit_string)\n",
    "    return \"\".join(bits) # return as one string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0WS86GPURFK5|76KUS3QCGVCY - c68686868e0f0e1c|c78786868e0f0e1c - 2\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# key value pair (key, value)\n",
    "\n",
    "example_hash_strings= [(\"0WS86GPURFK5\",\"c68686868e0f0e1c\"), \n",
    "                    (\"76KUS3QCGVCY\",\"c78786868e0f0e1c\") ,\n",
    "                    (\"96EC4QS20Z28\",\"c78786868e0f0e1c\"),\n",
    "                    (\"CL8W7L333U90\",\"c78706868e0f0e1c\"),\n",
    "                    (\"FDAZ5NL5NFL2\",\"c7870646ce0f0e1c\"),\n",
    "                    (\"HBX8QLI9HH25\",\"c7870686ce0f0e1c\"),\n",
    "                    (\"JY2ZAINWD2RX\",\"c68706068e0e0e1c\"),\n",
    "                    (\"LP47ZGJ256YU\",\"c78786068e0f0e1e\"),\n",
    "                    (\"NTETO8P77N96\",\"c78786868e0f0e1c\"),\n",
    "                    (\"SLK2PRXGW3DZ\",\"c78706868e0f0e1c\")]\n",
    "\n",
    "example_hash_strings2= [(\"0WS86GPURFK5\",\"c68686868e0f0e1c\"), \n",
    "                    (\"76KUS3QCGVCY\",\"c78786868e0f0e1c\") ]\n",
    "\n",
    "\n",
    "find_hamming_distances(example_hash_strings2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Result hond 1\n",
    "\n",
    "--------------------------------------\n",
    "Video frame length: 311\n",
    "File name: 0WS86GPURFK5 \n",
    "c6 86 86 86 8e 0f 0e 1c \n",
    "--------------------------------------\n",
    "Video frame length: 308\n",
    "File name: 76KUS3QCGVCY \n",
    "c7 87 86 86 8e 0f 0e 1c \n",
    "--------------------------------------\n",
    "Video frame length: 259\n",
    "File name: 96EC4QS20Z28 \n",
    "c7 87 86 86 8e 0f 0e 1c \n",
    "--------------------------------------\n",
    "Video frame length: 319\n",
    "File name: CL8W7L333U90 \n",
    "c7 87 06 86 8e 0f 0e 1c \n",
    "--------------------------------------\n",
    "Video frame length: 402\n",
    "File name: FDAZ5NL5NFL2 \n",
    "c7 87 06 46 ce 0f 0e 1c \n",
    "--------------------------------------\n",
    "Video frame length: 332\n",
    "File name: HBX8QLI9HH25 \n",
    "c7 87 06 86 ce 0f 0e 1c \n",
    "--------------------------------------\n",
    "Video frame length: 233\n",
    "File name: JY2ZAINWD2RX \n",
    "c6 87 06 06 8e 0e 0e 1c \n",
    "--------------------------------------\n",
    "Video frame length: 419\n",
    "File name: LP47ZGJ256YU \n",
    "c7 87 86 06 8e 0f 0e 1e \n",
    "--------------------------------------\n",
    "Video frame length: 448\n",
    "File name: NTETO8P77N96 \n",
    "c7 87 86 86 8e 0f 0e 1c \n",
    "--------------------------------------\n",
    "Video frame length: 282\n",
    "File name: SLK2PRXGW3DZ \n",
    "c7 87 06 86 8e 0f 0e 1c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature hashing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "hash_string = [\n",
    "            \"f3f3e3e34342c282\"\n",
    "            \"e7e7e74747048402\",\n",
    "            \"e7e7e7e7e7048402\",\n",
    "            \"e7e7e7e3e3000407\",\n",
    "            \"f7e7e3e3a3000407\",\n",
    "            \"f7f7f3e3e3810004\",\n",
    "            \"f7f7f3f3e3818005\",\n",
    "            \"e7e7f1f1f3018505\",\n",
    "            \"65e7e7e7e7214105\",\n",
    "            \"dff6e2e2e203000a\",\n",
    "            \"d7e6e6c6c662c00a\",\n",
    "            \"f6e6e4e4c426860c\",\n",
    "            \"f7e7c4848484c005\",\n",
    "            \"f7cfc0c0c0004088\",\n",
    "            \"f7c7c1c1c1414181\",\n",
    "            \"b7c3c3c3c3828282\",\n",
    "            \"674747c7c5850505\",\n",
    "            \"c7c38787c7c30303\",\n",
    "            \"8383838787878787\",\n",
    "            \"43c3470707878f0f\",\n",
    "            \"e3c7070707070f0f\",\n",
    "            \"733307070787070f\",\n",
    "            \"ff130307078707cf\",\n",
    "            \"ff878787c7878787\",\n",
    "            \"d7c3c3c347474343\",\n",
    "            \"e3e3c3c3c3030181\",\n",
    "            \"e7c183c3c3838380\",\n",
    "            \"f7e7c1c141c1c141\",\n",
    "            \"f5e3e1e161c1c000\",\n",
    "            \"faf1f1f141416140\",\n",
    "            \"fbf0f07041400549\",\n",
    "            \"fbf1f0b020602000\",\n",
    "            \"f3f0f0e020610000\",\n",
    "            \"f9f1f0f0a0e00000\",\n",
    "            \"fbbbf0f0d0304000\",\n",
    "            \"dffbf8f0f0905001\",\n",
    "            \"bffbf8f0d0104000\",\n",
    "            \"fdfdf0b08080c002\",\n",
    "            \"fdf5f4a484002004\",\n",
    "            \"b7b7e4e4c4a40408\",\n",
    "            \"f6f6e6c64242040d\",\n",
    "            \"e3f3f2e22203020e\",\n",
    "            \"f3f3e1c10203020e\",\n",
    "            \"f1f1e1c002030a0e\",\n",
    "            \"f3f2e2e020020206\",\n",
    "            \"f3f2e2e202020206\",\n",
    "            \"f3f3f2e020010002\",\n",
    "            \"f3f3f1e120018286\",\n",
    "            \"fbf3f1f1a181c101\",\n",
    "            \"e7fbf1f1f1a18104\",\n",
    "            \"f6f3f1f1f1610385\",\n",
    "            \"f3f3f1e1e181810c\",\n",
    "            \"ebf3f1e0a003020e\",\n",
    "            \"eff3f2e26242020a\",\n",
    "            \"fff3f2f040420208\",\n",
    "            \"f3f3f2e040010082\",\n",
    "            \"fbf3f2f240610000\",\n",
    "            \"fbfbf2f230110080\",\n",
    "            \"fbfbf9f0b0310000\",\n",
    "            \"ebf9f3e2a2330206\",\n",
    "            \"e3e3e1e223a30246\",\n",
    "            \"e1c1e1e223c30206\",\n",
    "            \"f9e1e1c001c30202\"]\n",
    "\n",
    "hasher = FeatureHasher(input_type='string', n_features=8)\n",
    "X = hasher.transform(hash_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -5.  0.  2.  5.  0.  0.  9.]\n",
      "[ 3. -3.  0.  0.  0.  0.  0.  6.]\n",
      "[ 3. -4.  0.  0.  2.  0.  0.  5.]\n",
      "[ 2. -4.  1.  1.  3.  0.  0.  3.]\n",
      "[ 1. -4.  0.  3.  3. -1.  0.  2.]\n",
      "[ 2. -4.  0.  4.  4. -1.  0.  1.]\n",
      "[ 2. -3.  0.  3.  3. -3.  0.  2.]\n",
      "[ 4. -1.  0.  0.  2. -2.  0.  5.]\n",
      "[ 1. -4.  1.  3.  1.  0.  0.  6.]\n",
      "[ 6. -5.  1.  1.  0.  0.  0.  3.]\n",
      "[ 1. -4.  0.  1.  0.  0.  0.  4.]\n",
      "[-2. -7.  0.  1.  1.  0.  0.  1.]\n",
      "[  0. -12.   0.   2.   0.   0.   0.   0.]\n",
      "[ 0. -5.  0.  1.  0. -6.  0.  0.]\n",
      "[ 1. -7.  0.  0.  4. -1.  0.  3.]\n",
      "[ 3. -5.  0.  0.  4.  0.  0.  0.]\n",
      "[ 4. -8.  0.  0.  4.  0.  0.  0.]\n",
      "[ 5. -8.  0.  0.  3.  0.  0.  0.]\n",
      "[ 2. -6.  0.  2.  2.  0.  0.  0.]\n",
      "[ 5. -7.  0.  2.  1.  0.  0.  1.]\n",
      "[ 6. -6.  0.  1.  3.  0.  0.  0.]\n",
      "[ 4. -6.  0.  3.  2. -1.  0.  0.]\n",
      "[ 7. -7.  0.  2.  0.  0.  0.  0.]\n",
      "[-1. -3.  0.  1.  5.  0.  0.  0.]\n",
      "[ 0. -6.  0.  0.  6. -2.  0.  2.]\n",
      "[ 1. -8.  0.  0.  5. -1.  0.  1.]\n",
      "[ 0. -4.  0.  1.  0. -6.  0.  1.]\n",
      "[ 1. -5.  0.  1.  2. -4.  0.  3.]\n",
      "[-2. -1.  1.  4.  0. -6.  0.  0.]\n",
      "[-2. -4.  0.  3.  1. -2.  0.  0.]\n",
      "[ 1. -7.  0.  3.  0. -3.  0.  2.]\n",
      "[ 1. -8.  0.  3.  1. -1.  0.  2.]\n",
      "[ 0. -7.  1.  4.  0. -1.  0.  1.]\n",
      "[-1. -7.  0.  4.  1. -3.  0.  0.]\n",
      "[ 0. -5.  0.  6.  1. -2.  0.  0.]\n",
      "[-1. -7.  0.  5.  0. -3.  0.  0.]\n",
      "[ 0. -9.  0.  5.  0. -1.  0.  1.]\n",
      "[-4. -5.  1.  4.  1.  0.  0.  1.]\n",
      "[-3. -4.  1.  0.  0. -2.  0.  2.]\n",
      "[ 1. -3.  0.  3.  0.  0.  0.  3.]\n",
      "[ 0. -3.  0.  2.  3.  0.  0.  8.]\n",
      "[ 0. -5.  0.  2.  3. -2.  0.  4.]\n",
      "[ 0. -6.  1.  2.  1. -3.  0.  3.]\n",
      "[ 1. -5.  0.  2.  1.  0.  0.  7.]\n",
      "[ 1. -4.  0.  2.  1.  0.  0.  8.]\n",
      "[ 0. -6.  0.  3.  2. -1.  0.  4.]\n",
      "[ 1. -4.  0.  3.  2. -3.  0.  3.]\n",
      "[ 0. -3.  1.  4.  1. -7.  0.  0.]\n",
      "[ 0. -2.  1.  4.  0. -6.  0.  1.]\n",
      "[ 2. -2.  0.  5.  3. -4.  0.  0.]\n",
      "[ 0. -4.  0.  3.  2. -5.  0.  2.]\n",
      "[ 0. -5.  1.  2.  2. -2.  0.  4.]\n",
      "[ 0. -2.  1.  3.  1.  0.  0.  7.]\n",
      "[-2. -5.  0.  5.  1.  0.  0.  3.]\n",
      "[-1. -6.  0.  3.  2. -1.  0.  3.]\n",
      "[ 0. -5.  0.  4.  1. -2.  0.  2.]\n",
      "[ 0. -5.  0.  4.  1. -4.  0.  2.]\n",
      "[ 0. -5.  0.  4.  1. -4.  0.  0.]\n",
      "[ 1. -1.  1.  2.  3. -1.  0.  5.]\n",
      "[ 0. -1.  1.  0.  4. -1.  0.  7.]\n",
      "[ 1. -4.  0.  0.  2. -3.  0.  6.]\n",
      "[ 0. -5.  0.  1.  1. -3.  0.  4.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in X.toarray():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bla = \"ebf9f3e2a2330206\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.388756690429\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "dataSetI = [1, 200, 3, 4]\n",
    "dataSetII = [1, 2, 3, 4]\n",
    "result = 1 - spatial.distance.cosine(dataSetI, dataSetII)\n",
    "\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eb', 'f9', 'f3', 'e2', 'a2', '33', '02', '06']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
