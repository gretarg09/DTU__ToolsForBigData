{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 10.1 Feature Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bag of words implementation\n",
      "--------------------------------\n",
      "     The score is 0.965317919075\n",
      "     The execution time was: 5.92660593987\n",
      "\n",
      "The feature hashing implementation\n",
      "     The score is 0.923410404624\n",
      "     The execution time was: 5.83922410011\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import json\n",
    "import time\n",
    "import mmh3\n",
    "\n",
    "# ------------------ Reading in data and preprocessing ----------------------\n",
    "\n",
    "path_to_folder = \"/Users/GretarAtli/Documents/GitHub/Dtu/Dtu-ToolsForBigData/week_10/ex10/data\"\n",
    "name = '/reuters-'\n",
    "\n",
    "file_paths = []\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(10):\n",
    "        if i== 2 and j > 1:\n",
    "            break\n",
    "    \n",
    "        file_paths.append(path_to_folder  + name + '0' + str(i) + str(j) + '.json')   \n",
    "\n",
    "topics = []\n",
    "bodies = []\n",
    "\n",
    "# Read in the data\n",
    "# we use try and catch to remove all articles that do not have at least one topic and a body\n",
    "# we append one to the topic array if earn is part of the topics of the article. \n",
    "# We append 0 otherwise\n",
    "for path in file_paths:\n",
    "    with open(path) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        for di in d:\n",
    "            try:\n",
    "                topic = di['topics']\n",
    "                body = di['body']\n",
    "                # Check if earn is part of the topics\n",
    "                if \"earn\" in topic:\n",
    "                    topics.append(1)\n",
    "                else:\n",
    "                    topics.append(0)\n",
    "                bodies.append(body)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "\n",
    "# -------------- The bag of word model ----------------------------\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Create raw bag-of-words encoding.\n",
    "cv = CountVectorizer(stop_words=\"english\", lowercase=True)\n",
    "bag_of_words_matrix = cv.fit_transform(bodies)\n",
    "bag_of_words_matrix.toarray()\n",
    "\n",
    "# Split the data into training and testing set\n",
    "x_train,x_test,y_train,y_test = train_test_split( bag_of_words_matrix, topics, test_size=0.2)\n",
    "\n",
    "# Now we are ready to use the random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf.fit(x_train, y_train)\n",
    "y_predicte = clf.predict(x_test)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "# Report the score\n",
    "print (\"The bag of words implementation\")\n",
    "print (\"--------------------------------\")\n",
    "print (\"     The score is {}\".format(\n",
    "    np.sum( np.array(y_test) == np.array(y_predicte) ) / len(np.array(y_test))\n",
    "    ))\n",
    "print (\"     The execution time was: {}\\n\".format(t2-t1))\n",
    "\n",
    "# ---------------------- Feature hashing model ----------------------\n",
    "# Now we implement feature hashing and use 1000 buckets instead of the raw bag-of-words encoding.\n",
    "\n",
    "# First we implement the hashing vectorizer\n",
    "# We both tried the build in hash function in python and the mmh3 hash() module in python\n",
    "# Both methods gave similar results. We chose to go with the build in function\n",
    "def hashing_vectorizer(features, N):\n",
    "    x = np.zeros(N)\n",
    "    for f in features:\n",
    "        h = hash(f)\n",
    "        #f = f.encode('utf-8')\n",
    "        #h = mmh3.hash(f)\n",
    "        x[h % N] += 1\n",
    "        \n",
    "    return x\n",
    "\n",
    "t3 = time.time()\n",
    "\n",
    "# initialize the feature hashing matrix\n",
    "N = 1000 # number of buckets\n",
    "feature_hasing_matrix = np.zeros((len(bodies), N))\n",
    "\n",
    "# create the feature hashing matrix\n",
    "for i,_ in enumerate(feature_hasing_matrix):\n",
    "    feature_hasing_matrix[i] = hashing_vectorizer(bodies[i],N)\n",
    "    \n",
    "# Know we use this matrix to train our machine learning algorithm\n",
    "\n",
    "# Split the data into training and testing set\n",
    "x_train,x_test,y_train,y_test = train_test_split( feature_hasing_matrix, topics, test_size=0.2)\n",
    "\n",
    "# Now we are ready to use the random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf.fit(x_train, y_train)\n",
    "y_predicte = clf.predict(x_test)\n",
    "\n",
    "t4 = time.time()\n",
    "\n",
    "# Report the score\n",
    "print(\"The feature hashing implementation\")\n",
    "print(\"     The score is {}\".format(\n",
    "    np.sum( np.array(y_test) == np.array(y_predicte) ) / len(np.array(y_test))\n",
    "    ))\n",
    "print (\"     The execution time was: {}\".format(t4-t3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10377x28151 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 594530 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bag_of_words_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10.2 (LSH)\n",
    "Implement the LSH algorithm from the lecture to hash images (so that similar images get similar hashes). Show that it works with an example or two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash for the figure cat.png:\n",
      "14 02 07 00 82 31 4b 03 \n",
      "The hash for the figure cat_mod.png:\n",
      "14 02 07 00 82 31 4b 03 \n",
      "The hash for the figure cat_mod_more.png:\n",
      "14 1a 07 10 82 31 4b 03 \n",
      "The hash for the figure cat_orange.png:\n",
      "00 20 44 41 43 b7 21 00 \n"
     ]
    }
   ],
   "source": [
    "import cv2 # for this I needed to install opencv -> pip install opencv-python\n",
    "import numpy as np\n",
    "\n",
    "def getHashLshAlgorithm(img):\n",
    "    \n",
    "    # Resize to 9x8 pixels\n",
    "    img = cv2.resize(img,(9,8))\n",
    "\n",
    "    # Compare adjacent values (x>y)\n",
    "    img_compared = np.empty((8,8))\n",
    "    for i,row in enumerate(img):\n",
    "        for j,column in enumerate(row):\n",
    "            if j + 1 == len(row):\n",
    "                break\n",
    "            else:\n",
    "                img_compared[i][j] = row[j] > row[j+1]\n",
    "\n",
    "    hash_string = \"\"\n",
    "    for difference in img_compared:\n",
    "        decimal_value = 0\n",
    "        hex_string = []\n",
    "        for index, value in enumerate(difference):\n",
    "            if value:\n",
    "                decimal_value += 2**(index % 8)\n",
    "            if (index % 8) == 7:\n",
    "                hex_string.append(hex(decimal_value)[2:].rjust(2, '0'))\n",
    "                decimal_value = 0\n",
    "        hash_string = hash_string +''.join(hex_string) + ' ' \n",
    "    return hash_string\n",
    "\n",
    "# Run the program for the following images \n",
    "filenames = ['cat.png', 'cat_mod.png','cat_mod_more.png','cat_orange.png']\n",
    "\n",
    "for name in filenames:\n",
    "    print \"The hash for the figure {}:\".format(name)\n",
    "    print getHashLshAlgorithm(cv2.imread(name, cv2.IMREAD_GRAYSCALE))\n",
    "\n",
    "\n",
    "# Grayscale the image\n",
    "#img_cat = cv2.imread('cat.png', cv2.IMREAD_GRAYSCALE)\n",
    "#img_cat_mod = cv2.imread('cat_mod.png', cv2.IMREAD_GRAYSCALE)\n",
    "#img_cat_mod_more = cv2.imread('cat_mod_more.png', cv2.IMREAD_GRAYSCALE)\n",
    "#img_cat_orange = cv2.imread('cat_orange.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4c 26 13 89 c4 62 31 98\n"
     ]
    }
   ],
   "source": [
    "differences = [[False, False, True, True, False, False, True, False],[False, True, True, False, False, True, False, False],[True, True, False, False, True, False, False, False],[True, False, False, True, False, False, False, True],[False, False, True, False, False, False, True, True],[False, True, False, False, False, True, True, False],[True, False, False, False, True, True, False, False],[False, False, False, True, True, False, False, True]]\n",
    "for difference in differences:\n",
    "\tdecimal_value = 0\n",
    "\thex_string = []\n",
    "\tfor index, value in enumerate(difference):\n",
    "\t\tif value:\n",
    "\t\t\tdecimal_value += 2**(index % 8)\n",
    "\t\tif (index % 8) == 7:\n",
    "\t\t\thex_string.append(hex(decimal_value)[2:].rjust(2, '0'))\n",
    "\t\t\tdecimal_value = 0\n",
    "\tprint ''.join(hex_string),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
